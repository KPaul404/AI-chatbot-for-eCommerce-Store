{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing relevant Libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk;\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer;\n",
    "from nltk.tokenize import word_tokenize;\n",
    "from nltk.tokenize import RegexpTokenizer;\n",
    "from nltk.corpus import stopwords;\n",
    "from nltk.stem import WordNetLemmatizer;\n",
    "nltk.download('wordnet');\n",
    "from nltk import bigrams, trigrams, FreqDist;\n",
    "nltk.download('stopwords');\n",
    "nltk.download('averaged_perceptron_tagger');\n",
    "nltk.download('punkt');\n",
    "from nltk.util import bigrams, trigrams\n",
    "from nltk.probability import FreqDist\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter;\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from IPython.display import Image, display\n",
    "# !pip install wordcloud;\n",
    "from scipy import stats\n",
    "import statistics\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import gzip\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing involves transforming raw data into a format suitable for analysis amd modelling which include transforming and structuring data to enhance its quality and utility, this will help in unlocking insights and building accurate and robust predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code parses and analyzes data from a compressed JSON file containing beauty-related questions and answers, extracting relevant columns such as question type, question text, and top answers, and then displaying them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      questionType                                       questionText  \\\n",
      "0       open-ended  May I request for a manual instruction on this...   \n",
      "1       open-ended  Having problems with the heater melting the wa...   \n",
      "2           yes/no  How much wax (in pounds, for instance) does it...   \n",
      "3       open-ended      How to know the expired date of this product?   \n",
      "4       open-ended  I am not in the sun as I work inside. I am new...   \n",
      "...            ...                                                ...   \n",
      "32931       yes/no              is Argan Oil Pure 100% good for Skin?   \n",
      "32932       yes/no  I find myself with rough cuticles right around...   \n",
      "32933       yes/no                        is it good for nail beauty?   \n",
      "32934   open-ended      how can i use it for Topical Use on Dry Hair?   \n",
      "32935   open-ended    how can i use it for Deep Conditioning Session?   \n",
      "\n",
      "                                              top_answer  \n",
      "0      Homedics has a complete list of instruction ma...  \n",
      "1      I leave the unit on all the time so it is alwa...  \n",
      "2      It came with the proper amount of wax, but thi...  \n",
      "3      The expiration date is on the bottom of the pl...  \n",
      "4      This product is awesome. I dont spend a lot of...  \n",
      "...                                                  ...  \n",
      "32931  Yes.... it is high in vitamin E and Essential ...  \n",
      "32932  Yes, you can. In the evening before you go to ...  \n",
      "32933  I would say it's good for cuticles.  I can't s...  \n",
      "32934  A little goes a long way!  A drop or two, depe...  \n",
      "32935  You can use it as a pre-shampoo treatment, whe...  \n",
      "\n",
      "[32936 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#Parsing and Analyzing Data from a Compressed JSON File\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "        \n",
    "def getDF(path):\n",
    "    data = list(parse(path)) \n",
    "    intent_df = pd.json_normalize(data, 'questions', ['asin'])\n",
    "    return intent_df\n",
    "\n",
    "intent_df = getDF('QA_Beauty.json.gz')\n",
    "\n",
    "# Keep only relevant columns\n",
    "intent_df = intent_df[['questionType', 'questionText', 'answers']]\n",
    "\n",
    "# Add column with top answer text\n",
    "intent_df['top_answer'] = intent_df['answers'].apply(lambda x: max(x, key=lambda y: float(y.get('answerScore', 0))).get('answerText', ''))\n",
    "\n",
    "# Display columns of interest\n",
    "print(intent_df[['questionType', 'questionText', 'top_answer']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code effectively aggregates questions and answers into a structured format and writes them to a JSON file for further processing\n",
    "The code aggregates questions and corresponding answers into a list of dictionaries under a generic tag named `\"General Inquiry\"`.\n",
    "The data is converted to a JSON-formatted string using the `json.dumps()` function, with indentation for better readability.\n",
    "The JSON-formatted string is written to a file named `'intents_data.json'`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate questions and answers into a list of dictionaries under a generic tag\n",
    "intent_data = {\n",
    "    \"tag\": \"General Inquiry\",\n",
    "    \"questions\": intent_df['questionText'].tolist(),\n",
    "    \"responses\": intent_df['top_answer'].tolist()\n",
    "}\n",
    "\n",
    "# Wrap the single tag data in a list to match the desired format\n",
    "intent_json = [intent_data]\n",
    "\n",
    "# Convert to JSON string\n",
    "intent_json_str = json.dumps(intent_json, indent=4)\n",
    "\n",
    "# Output the JSON string to a file\n",
    "with open('intents_data.json', 'w') as f:\n",
    "    f.write(intent_json_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below parses intent data from a JSON file, categorizes questions based on specific topics such as personal care, health and safety, electronics, and more, and then organizes them into a structured format with corresponding tags, questions, and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Personal Care', 'Accessories and Attachments', 'Electronics', 'Usage Instructions', 'Shipping and Packaging', 'Product Comparison', 'Warranty and Customer Support', 'Ingredient Specific Questions', 'International Shipping', 'Health and Safety', 'Fragrance', 'Product Authenticity'}\n"
     ]
    }
   ],
   "source": [
    "#Intent Data Processing from JSON File\n",
    "with open('intent_data.json') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "new_data = []\n",
    "tags = set()\n",
    "\n",
    "for item in data:\n",
    "    questions = item['questions']\n",
    "    \n",
    "    personal_care_questions = [q for q in questions if re.search(r'\\b(skin|sunblock|nail polish|beauty|hair)\\b', q, re.I)]\n",
    "    health_questions = [q for q in questions if re.search(r'\\b(allergies|irritation|hair loss|cancer|chemo|callus|ingredients? safety)\\b', q, re.I)]\n",
    "    electronics_questions = [q for q in questions if re.search(r'\\b(electrical cords?|batteries?|voltage|compatibility)\\b', q, re.I)]\n",
    "    shipping_questions = [q for q in questions if re.search(r'\\b(shipping|packaging|box|sealed|new)\\b', q, re.I)]\n",
    "    authenticity_questions = [q for q in questions if re.search(r'\\b(authenticity|counterfeit|compare|original)\\b', q, re.I)]\n",
    "    fragrance_questions = [q for q in questions if re.search(r'\\b(scent|fragrance|lasting|similar)\\b', q, re.I)] \n",
    "    instructions_questions = [q for q in questions if re.search(r'\\b(use|usage|instruction|direction|application|method|often)\\b', q, re.I)]\n",
    "    ingredients_questions = [q for q in questions if re.search(r'\\b(zinc|titanium|glycolic acid|lactic acid|formaldehyde|urea|BHT|alcohol)\\b', q, re.I)]\n",
    "    comparison_questions = [q for q in questions if re.search(r'\\b(compare|version|size|price)\\b', q, re.I)]\n",
    "    international_questions = [q for q in questions if re.search(r'\\b(international|country|region)\\b', q, re.I)]\n",
    "    accessories_questions = [q for q in questions if re.search(r'\\b(attachment|nozzle|cord|accessories)\\b', q, re.I)]\n",
    "    warranty_questions = [q for q in questions if re.search(r'\\b(warranty|return|refund|support)\\b', q, re.I)]\n",
    "    \n",
    "    # Tag assignment logic\n",
    "if personal_care_questions: \n",
    "    tags.add('Personal Care')\n",
    "    new_item = {'tag': 'Personal Care', 'questions': personal_care_questions, 'responses': item['responses']}\n",
    "    new_data.append(new_item)\n",
    "\n",
    "if health_questions:\n",
    "    tags.add('Health and Safety') \n",
    "    new_item = {'tag': 'Health and Safety', 'questions': health_questions, 'responses': item['responses']}\n",
    "    new_data.append(new_item)\n",
    "\n",
    "if electronics_questions:\n",
    "    tags.add('Electronics')\n",
    "    new_item = {'tag': 'Electronics', 'questions': electronics_questions, 'responses': item['responses']}\n",
    "    new_data.append(new_item)\n",
    "    \n",
    "if shipping_questions:\n",
    "    tags.add('Shipping and Packaging')\n",
    "    new_item = {'tag': 'Shipping and Packaging', 'questions': shipping_questions, 'responses': item['responses']}\n",
    "    new_data.append(new_item)\n",
    "\n",
    "if authenticity_questions:\n",
    "    tags.add('Product Authenticity')\n",
    "    new_item = {'tag': 'Product Authenticity', 'questions': authenticity_questions , 'responses': item['responses']}\n",
    "    new_data.append(new_item)\n",
    "    \n",
    "if fragrance_questions:\n",
    "    tags.add('Fragrance')\n",
    "    new_item = {'tag': 'Fragrance', 'questions': fragrance_questions, 'responses': item['responses']}\n",
    "    new_data.append(new_item)\n",
    "\n",
    "if instructions_questions:\n",
    "    tags.add('Usage Instructions')\n",
    "    new_item = {'tag': 'Usage Instructions', 'questions': instructions_questions, 'responses': item['responses']}\n",
    "    new_data.append(new_item)   \n",
    "\n",
    "if ingredients_questions:\n",
    "    tags.add('Ingredient Specific Questions')\n",
    "    new_item = {'tag': 'Ingredient Specific Questions', 'questions': ingredients_questions, 'responses': item['responses']}\n",
    "    new_data.append(new_item)\n",
    "\n",
    "if comparison_questions:\n",
    "    tags.add('Product Comparison')\n",
    "    new_item = {'tag': 'Product Comparison', 'questions': comparison_questions, 'responses': item['responses']}\n",
    "    new_data.append(new_item)\n",
    "\n",
    "if international_questions:\n",
    "    tags.add('International Shipping')\n",
    "    new_item = {'tag': 'International Shipping', 'questions': international_questions, 'responses': item['responses']}\n",
    "    new_data.append(new_item)  \n",
    "\n",
    "if accessories_questions:\n",
    "    tags.add('Accessories and Attachments')\n",
    "    new_item = {'tag': 'Accessories and Attachments', 'questions': accessories_questions, 'responses': item['responses']}\n",
    "    new_data.append(new_item)\n",
    "\n",
    "if warranty_questions:\n",
    "    tags.add('Warranty and Customer Support')\n",
    "    new_item = {'tag': 'Warranty and Customer Support', 'questions': warranty_questions, 'responses': item['responses']}\n",
    "    new_data.append(new_item)\n",
    "\n",
    "if not personal_care_questions and not health_questions and not electronics_questions and not shipping_questions and not authenticity_questions and not fragrance_questions and not instructions_questions and not ingredients_questions and not comparison_questions and not international_questions and not accessories_questions and not warranty_questions:\n",
    "    tags.add('General Inquiries')\n",
    "    new_item = {'tag': 'General Inquiries', 'questions': questions, 'responses': item['responses']}\n",
    "    new_data.append(new_item)\n",
    "    \n",
    "print(tags)\n",
    "# print(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code wraps the processed intent data into a list, converts it into a JSON string with indentation, and writes it to an output file named `'tagged_intent_data.json'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap new_data in a list \n",
    "output_data = [new_data] \n",
    "\n",
    "# Convert to JSON string\n",
    "output_json = json.dumps(output_data, indent=4)\n",
    "\n",
    "# Write to output file\n",
    "with open('tagged_intent_data.json', 'w') as f:\n",
    "    f.write(output_json)\n",
    "\n",
    "#print(output_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Categorized Intent Data from JSON File\n",
    "The code below reads categorized intent data from a JSON file named `'tagged_intent_data.json'`. It opens the file in read mode (`'r'`) using the `open()` function and loads the data into the variable `intents` using the `json.load()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data\n",
    "with open('tagged_intent_data.json', 'r') as file:\n",
    "    intents = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing\n",
    "The function `pair_questions_response` pairs questions with their corresponding responses from a list of dictionaries, assuming each sublist contains dictionaries with `'tag'`, `'questions'`, and `'responses'` keys, and returns a list of dictionaries with `'tag'`, `'question'`, and `'response'` keys.\n",
    "- `'tag'` represents the category of the product i.e personal care, electrical etc\n",
    "- `'question'` represents the question statement\n",
    "- `'response'` represents the answer statement to the given question\n",
    "\n",
    "After that it was then cleaned, normalized and tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pairing questions with responses\n",
    "def pair_questions_responses(data):\n",
    "    paired_data = []\n",
    "    for sublist in data:  # If data is a list of lists\n",
    "        for item in sublist:  # Assuming each sublist contains dictionaries\n",
    "            tag = item.get('tag', [])\n",
    "            questions = item.get('questions', [])\n",
    "            responses = item.get('responses', [])\n",
    "            \n",
    "            for question, response in zip(questions, responses):\n",
    "                paired_data.append({'tag': tag, 'question': question, 'response': response})\n",
    "    \n",
    "    return paired_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function to pair questions and responses\n",
    "paired_data = pair_questions_responses(intents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>question</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Personal Care</td>\n",
       "      <td>can i put this on a toddler skin?</td>\n",
       "      <td>Homedics has a complete list of instruction ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Personal Care</td>\n",
       "      <td>Does VANICREAM Mositurizing Skin Cream contain...</td>\n",
       "      <td>I leave the unit on all the time so it is alwa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Personal Care</td>\n",
       "      <td>Does it work for UV gel nail polish? It came u...</td>\n",
       "      <td>It came with the proper amount of wax, but thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Personal Care</td>\n",
       "      <td>Does your skin gets darker before it peels?</td>\n",
       "      <td>The expiration date is on the bottom of the pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Personal Care</td>\n",
       "      <td>Can this be used on neck, chest and hands too ...</td>\n",
       "      <td>This product is awesome. I dont spend a lot of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Personal Care</td>\n",
       "      <td>Will this help lighten/fade any acne scars? Do...</td>\n",
       "      <td>You could put it on a toddler's skin.. but not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Personal Care</td>\n",
       "      <td>It says for \"mature\" skin, I'm not sure what a...</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Personal Care</td>\n",
       "      <td>Do you have hair fiber?</td>\n",
       "      <td>It is not visible once applied.  I have been u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Personal Care</td>\n",
       "      <td>What color for Asian eyes? Hi, I usually don't...</td>\n",
       "      <td>Doesn't have one on the bottle.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Personal Care</td>\n",
       "      <td>Are 6 large rollers typically enough for an av...</td>\n",
       "      <td>I think so. The one for sensitive skin has zin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Personal Care</td>\n",
       "      <td>Will they work on chin length, thick hair?  Wo...</td>\n",
       "      <td>octinaxate  6% and zinc oxide % active ingredi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Personal Care</td>\n",
       "      <td>I used a Conair in a hotel and loved it becaus...</td>\n",
       "      <td>That is going to depend on the individual bott...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Personal Care</td>\n",
       "      <td>how long does it take to dry regular nail polish?</td>\n",
       "      <td>They have a plastic outer cover.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Personal Care</td>\n",
       "      <td>Does it make blonde, color-treated hair look b...</td>\n",
       "      <td>Yes, as I recall, it was in a non-specific box...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Personal Care</td>\n",
       "      <td>is it for thin hair</td>\n",
       "      <td>Small/petite generally fits 33\"-36\" and is adj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Personal Care</td>\n",
       "      <td>There are many cones in this product...how can...</td>\n",
       "      <td>Disposable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Personal Care</td>\n",
       "      <td>How do you use this product on short fine hair?</td>\n",
       "      <td>Yes the sizing is correct.  It will be a snug ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Personal Care</td>\n",
       "      <td>Will this product help my fine thin hair hold ...</td>\n",
       "      <td>yes.  I have been using their products for abo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Personal Care</td>\n",
       "      <td>Does this turn your skin \"bronze\" immediately ...</td>\n",
       "      <td>Very light beige I guess</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Personal Care</td>\n",
       "      <td>Is this an ionic hair dryer?  I don't want one...</td>\n",
       "      <td>I always follow with a moisturizer after a few...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tag                                           question  \\\n",
       "0   Personal Care                  can i put this on a toddler skin?   \n",
       "1   Personal Care  Does VANICREAM Mositurizing Skin Cream contain...   \n",
       "2   Personal Care  Does it work for UV gel nail polish? It came u...   \n",
       "3   Personal Care        Does your skin gets darker before it peels?   \n",
       "4   Personal Care  Can this be used on neck, chest and hands too ...   \n",
       "5   Personal Care  Will this help lighten/fade any acne scars? Do...   \n",
       "6   Personal Care  It says for \"mature\" skin, I'm not sure what a...   \n",
       "7   Personal Care                            Do you have hair fiber?   \n",
       "8   Personal Care  What color for Asian eyes? Hi, I usually don't...   \n",
       "9   Personal Care  Are 6 large rollers typically enough for an av...   \n",
       "10  Personal Care  Will they work on chin length, thick hair?  Wo...   \n",
       "11  Personal Care  I used a Conair in a hotel and loved it becaus...   \n",
       "12  Personal Care  how long does it take to dry regular nail polish?   \n",
       "13  Personal Care  Does it make blonde, color-treated hair look b...   \n",
       "14  Personal Care                                is it for thin hair   \n",
       "15  Personal Care  There are many cones in this product...how can...   \n",
       "16  Personal Care    How do you use this product on short fine hair?   \n",
       "17  Personal Care  Will this product help my fine thin hair hold ...   \n",
       "18  Personal Care  Does this turn your skin \"bronze\" immediately ...   \n",
       "19  Personal Care  Is this an ionic hair dryer?  I don't want one...   \n",
       "\n",
       "                                             response  \n",
       "0   Homedics has a complete list of instruction ma...  \n",
       "1   I leave the unit on all the time so it is alwa...  \n",
       "2   It came with the proper amount of wax, but thi...  \n",
       "3   The expiration date is on the bottom of the pl...  \n",
       "4   This product is awesome. I dont spend a lot of...  \n",
       "5   You could put it on a toddler's skin.. but not...  \n",
       "6                                                 Yes  \n",
       "7   It is not visible once applied.  I have been u...  \n",
       "8                     Doesn't have one on the bottle.  \n",
       "9   I think so. The one for sensitive skin has zin...  \n",
       "10  octinaxate  6% and zinc oxide % active ingredi...  \n",
       "11  That is going to depend on the individual bott...  \n",
       "12                   They have a plastic outer cover.  \n",
       "13  Yes, as I recall, it was in a non-specific box...  \n",
       "14  Small/petite generally fits 33\"-36\" and is adj...  \n",
       "15                                         Disposable  \n",
       "16  Yes the sizing is correct.  It will be a snug ...  \n",
       "17  yes.  I have been using their products for abo...  \n",
       "18                           Very light beige I guess  \n",
       "19  I always follow with a moisturizer after a few...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(paired_data)\n",
    "df.head(20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted list comprehension for nested structure\n",
    "normalized_questions = [question.lower() for sublist in intents for item in sublist for question in item['questions']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Questions (Sample): ['can i put this on a toddler skin', 'does vanicream mositurizing skin cream contain urea and what percentage', 'does it work for uv gel nail polish it came up in that search', 'does your skin gets darker before it peels', 'can this be used on neck chest and hands too or am i better off using a lower 5 solution for aging skin on those areas']\n"
     ]
    }
   ],
   "source": [
    "# Removing punctuation and special characters\n",
    "cleaned_questions = [re.sub(r'[^\\w\\s]', '', question) for question in normalized_questions]\n",
    "print(\"Cleaned Questions (Sample):\", cleaned_questions[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Questions (Sample): [['can', 'i', 'put', 'this', 'on', 'a', 'toddler', 'skin'], ['does', 'vanicream', 'mositurizing', 'skin', 'cream', 'contain', 'urea', 'and', 'what', 'percentage'], ['does', 'it', 'work', 'for', 'uv', 'gel', 'nail', 'polish', 'it', 'came', 'up', 'in', 'that', 'search'], ['does', 'your', 'skin', 'gets', 'darker', 'before', 'it', 'peels'], ['can', 'this', 'be', 'used', 'on', 'neck', 'chest', 'and', 'hands', 'too', 'or', 'am', 'i', 'better', 'off', 'using', 'a', 'lower', '5', 'solution', 'for', 'aging', 'skin', 'on', 'those', 'areas']]\n"
     ]
    }
   ],
   "source": [
    "# Tokenization: Break text into words\n",
    "tokenized_questions = [question.split() for question in cleaned_questions]\n",
    "# Display the first few tokenized questions\n",
    "print(\"Tokenized Questions (Sample):\", tokenized_questions[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Words (Sample): [['put', 'toddler', 'skin'], ['vanicream', 'mositurizing', 'skin', 'cream', 'contain', 'urea', 'percentage'], ['work', 'uv', 'gel', 'nail', 'polish', 'came', 'search'], ['skin', 'gets', 'darker', 'peels'], ['used', 'neck', 'chest', 'hands', 'better', 'using', 'lower', '5', 'solution', 'aging', 'skin', 'areas']]\n"
     ]
    }
   ],
   "source": [
    "# Stop words removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [[word for word in question if word not in stop_words] for question in tokenized_questions]\n",
    "# Display the first few sets of filtered words\n",
    "print(\"Filtered Words (Sample):\", filtered_words[:5])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizing Words using NLTK\n",
    "The code below utilizes the `WordNetLemmatizer` from the NLTK library to lemmatize words in a list of filtered words, and then prints a sample of the lemmatized words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Words (Sample): [['put', 'toddler', 'skin'], ['vanicream', 'mositurizing', 'skin', 'cream', 'contain', 'urea', 'percentage'], ['work', 'uv', 'gel', 'nail', 'polish', 'came', 'search'], ['skin', 'get', 'darker', 'peel'], ['used', 'neck', 'chest', 'hand', 'better', 'using', 'lower', '5', 'solution', 'aging', 'skin', 'area']]\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer() \n",
    "lemmatized_words = [[lemmatizer.lemmatize(word) for word in question] for question in filtered_words]\n",
    "\n",
    "print(\"Lemmatized Words (Sample):\", lemmatized_words[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Statistics on Intent Data\n",
    "The code computes the number of questions for each intent and stores it in a dictionary named `intent_counts`, where the intent tag is the key and the number of questions is the value.\n",
    "- Mean Questions per Intent: Calculates the mean number of questions per intent using the `mean()` function from the statistics module.\n",
    "- Median Questions per Intent: Calculates the median number of questions per intent using the `median()` function from the statistics module.\n",
    "- Total Intents: Computes the total number of unique intents.\n",
    "\n",
    "Find Intent(s) with the Fewest Questions:\n",
    "- Determines the minimum number of questions across all intents.\n",
    "- Identifies the intent(s) with the fewest questions based on the minimum count.\n",
    "\n",
    "The code prints the calculated statistics for analysis            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Intents: 12\n",
      "Mean Questions per Intent: 1151.92\n",
      "Median Questions per Intent: 475.0\n",
      "Intent(s) with Fewest Questions (75 questions): Health and Safety\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Adjusting to the nested list structure of intents\n",
    "flat_intents = [item for sublist in intents for item in sublist]  # Flatten the list of lists to a simple list of dictionaries\n",
    "\n",
    "# Compute statistics\n",
    "intent_counts = {item['tag']: len(item['questions']) for item in flat_intents}\n",
    "\n",
    "# Calculate additional statistics\n",
    "mean_questions_per_intent = statistics.mean(intent_counts.values())\n",
    "median_questions_per_intent = statistics.median(intent_counts.values())\n",
    "total_intents = len(intent_counts)\n",
    "\n",
    "# Find the intent(s) with the fewest questions\n",
    "min_questions_count = min(intent_counts.values())\n",
    "intents_with_fewest_questions = [intent for intent, count in intent_counts.items() if count == min_questions_count]\n",
    "\n",
    "# Print the calculated statistics\n",
    "print(f\"Total Intents: {total_intents}\")\n",
    "print(f\"Mean Questions per Intent: {mean_questions_per_intent:.2f}\")\n",
    "print(f\"Median Questions per Intent: {median_questions_per_intent}\")\n",
    "print(f\"Intent(s) with Fewest Questions ({min_questions_count} questions): {', '.join(intents_with_fewest_questions)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above reveals key statistics about the distribution of questions among different intents in a dataset. There are a total of 12 different intents covered. On average, each intent has about 1152 questions, with the middle value (median) being 475 questions per intent. Interestingly, the category labeled \"Health and Safety\" has the fewest questions, specifically 75. This information provides insights into the dataset's composition, indicating which intents receive more or less attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Statistics for Question and Response Lengths\n",
    "The code calculates various statistics for question and response lengths based on the flattened list of intents (`flat_intents`).\n",
    "The code flattens the list of lists (`intents`) into a simple list of dictionaries, storing it in the `flat_intents` variable.\n",
    "\n",
    "The code:\n",
    "- Computes the length of each question (in terms of the number of words) for all intents and stores them in the question_lengths list.\n",
    "- Computes the length of each response (in terms of the number of words) for all intents and stores them in the response_lengths list.\n",
    "- It calculates the mean, median, and standard deviation of the question lengths using the mean(), median(), and stdev() functions from the statistics module.\n",
    "- It calculates the mean, median, and standard deviation of the response lengths using similar functions.\n",
    "The code prints out the calculated statistics for question lengths and response lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for Question Lengths:\n",
      "Mean: 18.869709903783548\n",
      "Median: 14\n",
      "Standard Deviation: 24.957750941256222\n",
      "\n",
      "\n",
      "Statistics for Response Lengths:\n",
      "Mean: 0.8106897685956129\n",
      "Median: 1.0\n",
      "Standard Deviation: 0.3917548590282223\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Flatten the list of lists to a simple list of dictionaries\n",
    "flat_intents = [item for sublist in intents for item in sublist]\n",
    "\n",
    "# Calculate the lengths of questions and responses for all intents\n",
    "question_lengths = [len(question.split()) for item in flat_intents for question in item['questions']]\n",
    "response_lengths = [len(response.split()) for item in flat_intents for responses in item['responses'] for response in responses]\n",
    "\n",
    "# Calculate the mean, median, and standard deviation for question lengths\n",
    "question_mean = statistics.mean(question_lengths)\n",
    "question_median = statistics.median(question_lengths)\n",
    "question_std_dev = statistics.stdev(question_lengths)\n",
    "\n",
    "# Calculate the mean, median, and standard deviation for response lengths\n",
    "response_mean = statistics.mean(response_lengths)\n",
    "response_median = statistics.median(response_lengths)\n",
    "response_std_dev = statistics.stdev(response_lengths)\n",
    "\n",
    "# Print the statistics\n",
    "print(\"Statistics for Question Lengths:\")\n",
    "print(f\"Mean: {question_mean}\")\n",
    "print(f\"Median: {question_median}\")\n",
    "print(f\"Standard Deviation: {question_std_dev}\")\n",
    "print(\"\\n\")\n",
    "print(\"Statistics for Response Lengths:\")\n",
    "print(f\"Mean: {response_mean}\")\n",
    "print(f\"Median: {response_median}\")\n",
    "print(f\"Standard Deviation: {response_std_dev}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above provides statistical information about the lengths of questions and responses in the dataset. For question lengths, the mean (average) length is approximately 18.87 words, with a median of 14 words. The standard deviation, which indicates the variability or spread of the data, is approximately 24.96 words. Similarly, for response lengths, the mean is about 0.81 words, the median is 1 word, and the standard deviation is around 0.39 words. These statistics offer insights into the typical lengths of questions and responses in the dataset, along with their variability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Question and Response Lengths using Plotly Express\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming flat_intents has been defined as shown in previous steps:\n",
    "flat_intents = [item for sublist in intents for item in sublist]\n",
    "\n",
    "# Calculate the lengths of questions and responses for all intents\n",
    "question_lengths = [len(question.split()) for item in flat_intents for question in item['questions']]\n",
    "response_lengths = [len(response.split()) for item in flat_intents for responses in item['responses'] for response in responses]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a histogram using Plotly Express\n",
    "fig = px.histogram(\n",
    "    x=question_lengths + response_lengths, \n",
    "    color=[\"Questions\"] * len(question_lengths) + [\"Responses\"] * len(response_lengths),\n",
    "    barmode='overlay',\n",
    "    opacity=0.6\n",
    ")\n",
    "\n",
    "# Update layout for the chart\n",
    "fig.update_layout(\n",
    "    title=\"Question and Response Lengths\",\n",
    "    xaxis_title=\"Word Count\",\n",
    "    yaxis_title=\"Frequency\",\n",
    "    legend_title=\"Type\",\n",
    "    width=800,  # Set the width of the figure\n",
    "    height=600   # Set the height of the figure\n",
    ")\n",
    "\n",
    "# Update x-axis range to be from 0 to 500\n",
    "fig.update_xaxes(range=[0, 500])\n",
    "\n",
    "# Note: Adjust the path as needed for your environment\n",
    "# For Jupyter Notebooks or IPython environments to display directly\n",
    "fig.show()\n",
    "\n",
    "# # Optionally, to save and display the image as a static file:\n",
    "# # Ensure kaleido is installed for image export\n",
    "# # pip install -U kaleido\n",
    "\n",
    "# Define path for saving the image, considering the environment's file system\n",
    "image_path = \"/mnt/data/question_response_lengths_histogram.png\"\n",
    "fig.write_image(image_path)\n",
    "\n",
    "# # Display the image directly in the notebook\n",
    "# # Adjust the path if necessary based on your environment\n",
    "# display(Image(filename=image_path))\n",
    "\n",
    "# ###\n",
    "\n",
    "# Load the image\n",
    "img = mpimg.imread('/mnt/data/question_response_lengths_histogram.png')\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(img)\n",
    "plt.axis('off')  # Turn off axis numbers and ticks\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Displaying the length of questions and responses\n",
    "question_lengths = [len(question.split()) for intent in intents for question in intent['questions']]\n",
    "response_lengths = [len(response.split()) for intent in intents for response in intent.get('responses', [])]\n",
    "\n",
    "# Create a histogram using Plotly Express\n",
    "fig = px.histogram(\n",
    "    x=question_lengths + response_lengths, \n",
    "    color=[\"Questions\"] * len(question_lengths) + [\"Responses\"] * len(response_lengths),\n",
    "    barmode='overlay',\n",
    "    opacity=0.6\n",
    ")\n",
    "\n",
    "# Update layout for the chart\n",
    "fig.update_layout(\n",
    "    title=\"Question and Response Lengths\",\n",
    "    xaxis_title=\"Word Count\",\n",
    "    yaxis_title=\"Frequency\",\n",
    "    legend_title=\"Type\"\n",
    ")\n",
    "\n",
    "# Update x-axis range to be from 0 to 500\n",
    "fig.update_xaxes(range=[0, 500])\n",
    "\n",
    "# Save the figure as an image (PNG, for example)\n",
    "image_path = \"question_response_lengths_histogram.png\"\n",
    "fig.write_image(image_path)\n",
    "\n",
    "# Display the image directly in the notebook\n",
    "display(Image(filename=image_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph is a violin plot showing the distribution of word counts in questions and responses. Responses exhibit a wider and more varied distribution, indicating a broader range of word counts. There is a significant concentration around 500-1000 words, but also several responses extending up to 3500 words. The width of the plot for responses is broader at multiple points along the y-axis, highlighting that there are various common lengths for responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculating word frequency\n",
    "all_words = [word for question in lemmatized_words for word in question]  # Flatten the list\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "# Display the most common words\n",
    "print(\"Most Common Words:\", word_freq.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_words = word_freq.most_common(20)\n",
    "words, frequencies = zip(*most_common_words)\n",
    "\n",
    "# Create a bar chart using Plotly Express\n",
    "fig = px.bar(\n",
    "    x=words,\n",
    "    y=frequencies,\n",
    "    title=\"Top 20 Most Common Words in Questions\",\n",
    "    labels={'x': 'Words', 'y': 'Frequency'},\n",
    "    color_discrete_sequence=['skyblue']\n",
    ")\n",
    "\n",
    "# Update layout for the chart (adjusting size)\n",
    "fig.update_layout(\n",
    "    width=800,  # Set the width of the chart\n",
    "    height=600,  # Set the height of the chart\n",
    ")\n",
    "\n",
    "# Save the figure as an image (PNG, for example)\n",
    "image_path = \"most_common_words_bar_chart.png\"\n",
    "fig.write_image(image_path)\n",
    "\n",
    "# Display the image directly in the notebook\n",
    "display(Image(filename=image_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordcloud for the most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a word cloud for visualizing the most frequent words.\n",
    "# Generate a word cloud\n",
    "all_words_string = ' '.join([word for sublist in lemmatized_words for word in sublist])\n",
    "wordcloud = WordCloud(width=800, height=400, background_color ='white').generate(all_words_string)\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-grams and Tri-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install plotly\n",
    "# !pip install kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bi-grams and tri-grams\n",
    "bi_grams = list(bigrams(all_words))\n",
    "tri_grams = list(trigrams(all_words))\n",
    "\n",
    "# Frequency distribution\n",
    "bi_gram_freq = FreqDist(bi_grams)\n",
    "tri_gram_freq = FreqDist(tri_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bi_gram_labels, bi_gram_counts = zip(*bi_gram_freq.most_common(10))\n",
    "tri_gram_labels, tri_gram_counts = zip(*tri_gram_freq.most_common(10))\n",
    "bi_gram_labels_str = [' '.join(gram) for gram in bi_gram_labels]\n",
    "tri_gram_labels_str = [' '.join(gram) for gram in tri_gram_labels]\n",
    "\n",
    "# Initialize subplots\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Most Common Bi-grams\", \"Most Common Tri-grams\"))\n",
    "\n",
    "# Add bi-gram bar chart\n",
    "fig.add_trace(\n",
    "    go.Bar(x=bi_gram_labels_str, y=bi_gram_counts, name=\"Bi-grams\"),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Add tri-gram bar chart\n",
    "fig.add_trace(\n",
    "    go.Bar(x=tri_gram_labels_str, y=tri_gram_counts, name=\"Tri-grams\"),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title_text=\"Most Common N-grams\",\n",
    "    xaxis_title=\"N-grams\",\n",
    "    yaxis_title=\"Frequency\",\n",
    "    colorway=[\"skyblue\", \"salmon\"], \n",
    "    width=1000,  # Set the width of the chart\n",
    "    height=500,  # Set the height of the chart\n",
    ")\n",
    "\n",
    "# Save the figure as an image \n",
    "image_path = \"most_common_ngrams_subplot.png\"\n",
    "fig.write_image(image_path)\n",
    "\n",
    "# Display the image directly in the notebook\n",
    "display(Image(filename=image_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
